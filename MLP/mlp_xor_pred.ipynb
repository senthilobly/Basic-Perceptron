{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Multi Layer Perceptron for XOR Gate Prediction"
      ],
      "metadata": {
        "id": "2p9r0VSU3jqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulating the Xor Gate prediction using binary values as mentioned in X.\n",
        "X with 4 Samples and 2 Features.\n",
        "\n",
        "1. No of Input layers = 1 layer with 2 Neuron (Features)\n",
        "2. Hidden layer = 1 layer with 2 Neuron\n",
        "3. Output layer = 1 layer with 1 Neuron\n",
        "\n",
        "Multiple Layer Perceptron for XOR gate Prediciton\n",
        "\n",
        "X is the input with 4x2 matrix which resembles 4 samples and 2 input features\n",
        "\n",
        "Y is the true output\n",
        "\n",
        "We use 2 hidden layer each with 4 neuron and 2 Bias parameter\n",
        "\n",
        "Simulating the Xor Gate prediction using binary values as mentioned in X. X with 4 Samples and 2 Features.\n",
        "\n",
        "No of Input layers = 1 layer with 2 Neuron (Features)\n",
        "Hidden layer = 1 layer with 2 Neuron\n",
        "Output layer = 1 layer with 1 Neuron\n",
        "Gradient Calculation\n",
        "\n",
        "Loss Gradient = dLoss/daout using cross-entropy derivative weight_gradients = (np.dot(prev_layer.T, dz)) Bias Gradients = Sum of dz's\n",
        "\n",
        "dz = dLoss_from_next_layer * activation_derivative(current_layer_output) dw = np.dot(previous_layer_output.T, dz)"
      ],
      "metadata": {
        "id": "w5WS_oXa5u8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.array([[0,0], [1,0], [1,1], [0,1]])  # shape (4,2)\n",
        "Y = np.array([[0], [1], [0], [1]])          # shape (4,1)\n",
        "\n",
        "np.random.seed(42)  # reproducibility\n",
        "\n",
        "W1 = np.random.randn(2, 4)\n",
        "b1 = np.random.randn(1, 4)\n",
        "\n",
        "W2 = np.random.randn(4, 4)\n",
        "b2 = np.zeros((1, 4))\n",
        "\n",
        "W3 = np.random.randn(4, 1)\n",
        "b3 = np.zeros((1, 1))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = relu(Z2)\n",
        "\n",
        "    Z3 = np.dot(A2, W3) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    cache = (X, Z1, A1, Z2, A2, Z3, A3)\n",
        "    return A3, cache\n",
        "\n",
        "def compute_loss(Y, A3):\n",
        "    m = Y.shape[0]\n",
        "    loss = - (np.sum(Y * np.log(A3) + (1 - Y) * np.log(1 - A3))) / m\n",
        "    return loss\n",
        "\n",
        "def backward_propagation(Y, cache, W2, W3, loss):\n",
        "    X, Z1, A1, Z2, A2, Z3, A3 = cache\n",
        "    m = Y.shape[0]\n",
        "\n",
        "    #dZ3 = loss * sigmoid_derivative(A3)\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = np.dot(A2.T, dZ3) / m\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "    dZ2 = np.dot(dZ3, W3.T) * relu_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dZ1 = np.dot(dZ2, W2.T) * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    gradients = (dW1, db1, dW2, db2, dW3, db3)\n",
        "    return gradients\n",
        "\n",
        "def update_parameters(W1, b1, W2, b2, W3, b3, gradients, lr):\n",
        "    dW1, db1, dW2, db2, dW3, db3 = gradients\n",
        "    W1 -= lr * dW1\n",
        "    b1 -= lr * db1\n",
        "    W2 -= lr * dW2\n",
        "    b2 -= lr * db2\n",
        "    W3 -= lr * dW3\n",
        "    b3 -= lr * db3\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    A3, cache = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
        "    loss = compute_loss(Y, A3)\n",
        "    gradients = backward_propagation(Y, cache, W2, W3, loss)\n",
        "    W1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, gradients, learning_rate)\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch} Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l_p08TY0peV",
        "outputId": "ecf693d4-d115-43da-f144-1325c09c8729"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 0.7099\n",
            "Epoch 1000 Loss: 0.0148\n",
            "Epoch 2000 Loss: 0.0065\n",
            "Epoch 3000 Loss: 0.0041\n",
            "Epoch 4000 Loss: 0.0030\n",
            "Epoch 5000 Loss: 0.0024\n",
            "Epoch 6000 Loss: 0.0019\n",
            "Epoch 7000 Loss: 0.0017\n",
            "Epoch 8000 Loss: 0.0014\n",
            "Epoch 9000 Loss: 0.0013\n"
          ]
        }
      ]
    }
  ]
}